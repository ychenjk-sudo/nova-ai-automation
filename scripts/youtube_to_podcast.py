#!/usr/bin/env python3
"""
YouTube ç¿»è¯‘ â†’ æ’­å®¢ è‡ªåŠ¨åŒ–æµæ°´çº¿

æµç¨‹ï¼š
1. ä»Ž youtube-translations ä»“åº“è¯»å–ç¿»è¯‘æ–‡æ¡£
2. AI ç”Ÿæˆæ’­å®¢è„šæœ¬ï¼ˆå¯¹è¯ä½“ï¼‰
3. TTS ç”ŸæˆéŸ³é¢‘
4. ä¸Šä¼ åˆ° lixiang-podcast ä»“åº“
5. æ›´æ–° RSS Feed
"""

import os
import re
import json
import subprocess
from datetime import datetime
from pathlib import Path

# ============ é…ç½® ============

# ä»“åº“è·¯å¾„
TRANSLATIONS_REPO = "/tmp/yt-trans"
PODCAST_REPO = "/workspace/NovaAI-Podcast"

# æ’­å®¢é…ç½®
PODCAST_CONFIG = {
    "title": "AIå‰æ²¿è§£è¯»",
    "description": "æ¯æœŸç²¾é€‰ä¸€ä¸ª YouTube æ·±åº¦è®¿è°ˆï¼Œç”¨ä¸­æ–‡ä¸ºä½ è§£è¯» AI é¢†åŸŸæœ€å‰æ²¿çš„æ€æƒ³å’Œå®žè·µã€‚",
    "author": "ç†æƒ³",
    "email": "podcast@example.com",
    "language": "zh-cn",
    "category": "Technology",
}

# å·²å¤„ç†çš„æ–‡æ¡£è®°å½•
PROCESSED_FILE = f"{PODCAST_REPO}/processed.json"

# ============ æ ¸å¿ƒå‡½æ•° ============

def load_processed():
    """åŠ è½½å·²å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        with open(PROCESSED_FILE, 'r') as f:
            return json.load(f)
    except:
        return {"processed": []}

def save_processed(data):
    """ä¿å­˜å·²å¤„ç†åˆ—è¡¨"""
    with open(PROCESSED_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def get_translation_files():
    """èŽ·å–æ‰€æœ‰ç¿»è¯‘æ–‡æ¡£"""
    files = []
    for f in Path(TRANSLATIONS_REPO).glob("*.md"):
        if f.name != "README.md":
            files.append(f)
    return sorted(files, reverse=True)  # æœ€æ–°çš„åœ¨å‰

def parse_translation(file_path):
    """è§£æžç¿»è¯‘æ–‡æ¡£"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
    title = title_match.group(1) if title_match else file_path.stem
    
    # æå–è§†é¢‘é“¾æŽ¥
    link_match = re.search(r'\*\*è§†é¢‘é“¾æŽ¥\*\*: (.+)$', content, re.MULTILINE)
    video_link = link_match.group(1) if link_match else ""
    
    # æå–æ ¸å¿ƒè§‚ç‚¹
    core_points = []
    points_section = re.search(r'## æ ¸å¿ƒè§‚ç‚¹\n(.+?)(?=\n## |\n---|\Z)', content, re.DOTALL)
    if points_section:
        points_text = points_section.group(1)
        # æå–æ¯ä¸ªè¦ç‚¹çš„æ ‡é¢˜
        for match in re.finditer(r'### \d+\. (.+)\n(.+?)(?=\n### |\Z)', points_text, re.DOTALL):
            core_points.append({
                "title": match.group(1),
                "content": match.group(2).strip()[:500]
            })
    
    # æå–é¢‘é“å’Œæ—¥æœŸ
    channel_match = re.search(r'\*\*é¢‘é“\*\*: (.+)$', content, re.MULTILINE)
    channel = channel_match.group(1) if channel_match else "Unknown"
    
    date_match = re.search(r'\*\*å‘å¸ƒæ—¶é—´\*\*: (.+)$', content, re.MULTILINE)
    pub_date = date_match.group(1) if date_match else ""
    
    return {
        "filename": file_path.name,
        "title": title,
        "video_link": video_link,
        "channel": channel,
        "pub_date": pub_date,
        "core_points": core_points,
        "content": content
    }

def generate_podcast_script(doc):
    """ç”Ÿæˆæ’­å®¢è„šæœ¬ï¼ˆå¯ä»¥ç”¨ AI ä¼˜åŒ–ï¼‰"""
    
    script = f"""
å¤§å®¶å¥½ï¼Œæ¬¢è¿Žæ”¶å¬ã€ŒAIå‰æ²¿è§£è¯»ã€ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ä¸»æ’­ã€‚

ä»Šå¤©è¿™æœŸèŠ‚ç›®ï¼Œæˆ‘ä»¬æ¥èŠä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„è§†é¢‘ï¼š{doc['title']}

è¿™ä¸ªè§†é¢‘æ¥è‡ª {doc['channel']} é¢‘é“ï¼ŒåŽŸè§†é¢‘é“¾æŽ¥æˆ‘ä¼šæ”¾åœ¨èŠ‚ç›®ç®€ä»‹é‡Œã€‚

å¥½ï¼Œæˆ‘ä»¬ç›´æŽ¥è¿›å…¥æ­£é¢˜ã€‚è¿™ä¸ªè§†é¢‘ä¸»è¦è®²äº†ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒè§‚ç‚¹ï¼š

"""
    
    for i, point in enumerate(doc['core_points'][:5], 1):
        script += f"""
ç¬¬{i}ç‚¹ï¼Œ{point['title']}ã€‚

{point['content'][:300]}

"""
    
    script += """
å¥½äº†ï¼Œä»¥ä¸Šå°±æ˜¯ä»Šå¤©è¿™æœŸèŠ‚ç›®çš„ä¸»è¦å†…å®¹ã€‚

å¦‚æžœä½ è§‰å¾—æœ‰æ”¶èŽ·ï¼Œæ¬¢è¿Žè®¢é˜…æˆ‘ä»¬çš„æ’­å®¢ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼
"""
    
    return script.strip()

def text_to_speech_edge(text, output_path):
    """ä½¿ç”¨ Edge TTS ç”ŸæˆéŸ³é¢‘ï¼ˆå…è´¹ï¼‰"""
    import asyncio
    import edge_tts
    
    async def generate():
        communicate = edge_tts.Communicate(text, "zh-CN-YunxiNeural")
        await communicate.save(output_path)
    
    asyncio.run(generate())
    print(f"âœ… éŸ³é¢‘å·²ç”Ÿæˆ: {output_path}")

def update_podcast_rss():
    """æ›´æ–°æ’­å®¢ RSS"""
    # è°ƒç”¨ä¹‹å‰å†™çš„è„šæœ¬
    os.system(f"cd {PODCAST_REPO} && python /workspace/scripts/podcast_github_rss.py generate 2>/dev/null")

def git_push_podcast(message):
    """æŽ¨é€æ’­å®¢æ›´æ–°"""
    os.chdir(PODCAST_REPO)
    subprocess.run(["git", "add", "."], check=True)
    subprocess.run(["git", "commit", "-m", message], check=True)
    subprocess.run(["git", "push"], check=True)
    print(f"âœ… å·²æŽ¨é€åˆ° GitHub")

def process_one_translation(file_path):
    """å¤„ç†å•ä¸ªç¿»è¯‘æ–‡æ¡£"""
    print(f"\nðŸ“„ å¤„ç†: {file_path.name}")
    
    # è§£æžæ–‡æ¡£
    doc = parse_translation(file_path)
    print(f"   æ ‡é¢˜: {doc['title']}")
    print(f"   è¦ç‚¹æ•°: {len(doc['core_points'])}")
    
    # ç”Ÿæˆæ’­å®¢è„šæœ¬
    script = generate_podcast_script(doc)
    print(f"   è„šæœ¬é•¿åº¦: {len(script)} å­—")
    
    # ç”ŸæˆéŸ³é¢‘
    episode_id = f"ep{datetime.now().strftime('%Y%m%d%H%M')}"
    audio_filename = f"{episode_id}_{file_path.stem}.mp3"
    audio_path = f"{PODCAST_REPO}/episodes/{audio_filename}"
    
    os.makedirs(f"{PODCAST_REPO}/episodes", exist_ok=True)
    
    print(f"   æ­£åœ¨ç”ŸæˆéŸ³é¢‘...")
    text_to_speech_edge(script, audio_path)
    
    # æ›´æ–° episodes.json
    episodes_file = f"{PODCAST_REPO}/episodes.json"
    try:
        with open(episodes_file, 'r') as f:
            data = json.load(f)
    except:
        data = {"episodes": []}
    
    episode = {
        "id": episode_id,
        "title": f"è§£è¯»ï¼š{doc['title'][:50]}",
        "description": f"æœ¬æœŸè§£è¯» {doc['channel']} çš„è§†é¢‘ã€‚åŽŸè§†é¢‘ï¼š{doc['video_link']}",
        "filename": audio_filename,
        "audio_url": f"https://ychenjk-sudo.github.io/NovaAI-Podcast/episodes/{audio_filename}",
        "duration": 0,
        "file_size": os.path.getsize(audio_path),
        "pub_date": datetime.now().strftime("%a, %d %b %Y %H:%M:%S +0800"),
        "guid": episode_id,
        "source_file": file_path.name,
        "video_link": doc['video_link']
    }
    
    data["episodes"].insert(0, episode)
    
    with open(episodes_file, 'w') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return episode

def main():
    """ä¸»æµç¨‹"""
    print("ðŸŽ™ï¸ YouTube ç¿»è¯‘ â†’ æ’­å®¢ è‡ªåŠ¨åŒ–")
    print("=" * 50)
    
    # æ›´æ–°ç¿»è¯‘ä»“åº“
    print("\nðŸ“¥ æ›´æ–°ç¿»è¯‘ä»“åº“...")
    os.chdir(TRANSLATIONS_REPO)
    subprocess.run(["git", "pull"], capture_output=True)
    
    # åŠ è½½å·²å¤„ç†åˆ—è¡¨
    processed = load_processed()
    processed_files = set(processed.get("processed", []))
    
    # èŽ·å–æ‰€æœ‰ç¿»è¯‘æ–‡ä»¶
    files = get_translation_files()
    print(f"   æ‰¾åˆ° {len(files)} ä¸ªç¿»è¯‘æ–‡æ¡£")
    
    # æ‰¾å‡ºæœªå¤„ç†çš„
    new_files = [f for f in files if f.name not in processed_files]
    print(f"   å…¶ä¸­ {len(new_files)} ä¸ªæœªå¤„ç†")
    
    if not new_files:
        print("\nâœ… æ²¡æœ‰æ–°çš„ç¿»è¯‘éœ€è¦å¤„ç†")
        return
    
    # å¤„ç†ç¬¬ä¸€ä¸ªæ–°æ–‡æ¡£
    file_to_process = new_files[0]
    episode = process_one_translation(file_to_process)
    
    # æ ‡è®°ä¸ºå·²å¤„ç†
    processed["processed"].append(file_to_process.name)
    save_processed(processed)
    
    # æ›´æ–° RSS
    print("\nðŸ“» æ›´æ–° RSS Feed...")
    update_podcast_rss()
    
    # æŽ¨é€
    print("\nðŸš€ æŽ¨é€åˆ° GitHub...")
    git_push_podcast(f"Add podcast: {episode['title']}")
    
    print(f"\nðŸŽ‰ å®Œæˆï¼")
    print(f"ðŸ“» RSS: https://ychenjk-sudo.github.io/lixiang-podcast/feed.xml")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "list":
        # åˆ—å‡ºæ‰€æœ‰ç¿»è¯‘
        files = get_translation_files()
        for f in files:
            print(f.name)
    else:
        main()

#!/usr/bin/env python3
"""
AK (Karpathy) æ¨èçš„ 92 ä¸ª HN çƒ­é—¨åšå®¢ â†’ é£ä¹¦æ¨é€
"""

import feedparser
import json
import re
from datetime import datetime, timedelta
from html import unescape
import hashlib
import concurrent.futures
import time

# AK æ¨èçš„ OPML åœ°å€
OPML_URL = "https://gist.githubusercontent.com/emschwartz/e6d2bf860ccc367fe37ff953ba6de66b/raw/426957f043dc0054f95aae6c19de1d0b4ecc2bb2/hn-popular-blogs-2025.opml"
STATE_FILE = "/workspace/scripts/.ak_blogs_state.json"
MAX_ITEMS = 5  # æ¯æ¬¡æœ€å¤šæ¨é€å‡ æ¡
MAX_AGE_DAYS = 3  # åªçœ‹æœ€è¿‘å‡ å¤©çš„æ–‡ç« 

# é¢„å®šä¹‰çš„ RSS æºåˆ—è¡¨ï¼ˆä» OPML æå–ï¼‰
RSS_FEEDS = [
    ("Simon Willison", "https://simonwillison.net/atom/everything/"),
    ("Jeff Geerling", "https://www.jeffgeerling.com/blog.xml"),
    ("Krebs on Security", "https://krebsonsecurity.com/feed/"),
    ("Daring Fireball", "https://daringfireball.net/feeds/main"),
    ("antirez", "http://antirez.com/rss"),
    ("Pluralistic", "https://pluralistic.net/feed/"),
    ("Mitchell Hashimoto", "https://mitchellh.com/feed.xml"),
    ("Dynomight", "https://dynomight.net/feed.xml"),
    ("Xe Iaso", "https://xeiaso.net/blog.rss"),
    ("Old New Thing", "https://devblogs.microsoft.com/oldnewthing/feed"),
    ("Ken Shirriff", "https://www.righto.com/feeds/posts/default"),
    ("Armin Ronacher", "https://lucumr.pocoo.org/feed.atom"),
    ("Gary Marcus", "https://garymarcus.substack.com/feed"),
    ("Rachel by the Bay", "https://rachelbythebay.com/w/atom.xml"),
    ("Dan Abramov", "https://overreacted.io/rss.xml"),
    ("John D Cook", "https://www.johndcook.com/blog/feed/"),
    ("matklad", "https://matklad.github.io/feed.xml"),
    ("Evan Hahn", "https://evanhahn.com/feed.xml"),
    ("Terrible Software", "https://terriblesoftware.org/feed/"),
    ("Paul Graham", "http://www.aaronsw.com/2002/feeds/pgessays.rss"),
    ("Julia Evans", "https://jvns.ca/atom.xml"),
    ("Stratechery", "https://stratechery.com/feed/"),
    ("Hillel Wayne", "https://www.hillelwayne.com/index.xml"),
    ("fasterthanli.me", "https://fasterthanli.me/index.xml"),
    ("Drew DeVault", "https://drewdevault.com/blog/index.xml"),
    ("Molly White", "https://www.citationneeded.news/rss/"),
    ("Lenny's Newsletter", "https://www.lennysnewsletter.com/feed"),
    ("lcamtuf", "https://lcamtuf.substack.com/feed"),
    ("Ben Thompson", "https://stratechery.com/feed/"),
    ("Coding Horror", "https://blog.codinghorror.com/rss/"),
    ("Hacker News", "https://hnrss.org/frontpage"),
]

def load_state():
    try:
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    except:
        return {"sent_ids": [], "last_run": None}

def save_state(state):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)

def clean_html(html_text):
    if not html_text:
        return ""
    text = re.sub(r'<[^>]+>', '', str(html_text))
    text = unescape(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def parse_date(date_str):
    """å°è¯•è§£ææ—¥æœŸ"""
    if not date_str:
        return None
    try:
        # feedparser é€šå¸¸ä¼šè§£ææˆ time tuple
        import email.utils
        parsed = email.utils.parsedate_to_datetime(date_str)
        return parsed
    except:
        pass
    return None

def fetch_single_feed(feed_info):
    """æŠ“å–å•ä¸ª RSS æº"""
    name, url = feed_info
    try:
        import socket
        socket.setdefaulttimeout(5)  # 5ç§’è¶…æ—¶
        feed = feedparser.parse(url)
        articles = []
        cutoff = datetime.now() - timedelta(days=MAX_AGE_DAYS)
        
        for entry in feed.entries[:5]:  # æ¯ä¸ªæºæœ€å¤šå–5æ¡
            # è§£æå‘å¸ƒæ—¶é—´
            pub_date = None
            for date_field in ['published', 'updated', 'created']:
                if hasattr(entry, date_field + '_parsed') and getattr(entry, date_field + '_parsed'):
                    try:
                        pub_date = datetime(*getattr(entry, date_field + '_parsed')[:6])
                        break
                    except:
                        pass
            
            # è¿‡æ»¤å¤ªè€çš„æ–‡ç« 
            if pub_date and pub_date.replace(tzinfo=None) < cutoff:
                continue
            
            title = entry.get('title', '')
            link = entry.get('link', '')
            summary = clean_html(entry.get('summary', entry.get('description', '')))[:200]
            
            if title and link:
                articles.append({
                    "id": link,
                    "title": title,
                    "link": link,
                    "source": name,
                    "summary": summary + "..." if len(summary) >= 200 else summary,
                    "pub_date": pub_date.isoformat() if pub_date else None
                })
        
        return articles
    except Exception as e:
        return []

def fetch_all_feeds():
    """å¹¶å‘æŠ“å–æ‰€æœ‰æº"""
    all_articles = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_single_feed, RSS_FEEDS)
        for articles in results:
            all_articles.extend(articles)
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_articles.sort(key=lambda x: x.get('pub_date') or '', reverse=True)
    
    return all_articles

def format_feishu_message(articles):
    if not articles:
        return None
    
    today = datetime.now().strftime("%Y-%m-%d")
    
    lines = [f"ğŸ“š **AKæ¨èåšå®¢ç²¾é€‰** ({today})", ""]
    
    for i, art in enumerate(articles, 1):
        lines.append(f"**{i}. {art['title']}**")
        lines.append(f"   âœï¸ {art['source']}")
        if art['summary']:
            lines.append(f"   {art['summary'][:100]}...")
        lines.append(f"   ğŸ”— {art['link']}")
        lines.append("")
    
    lines.append("---")
    lines.append("_æ¥æº: Karpathy æ¨èçš„ 92 ä¸ª HN çƒ­é—¨åšå®¢_")
    
    return "\n".join(lines)

def main():
    state = load_state()
    sent_ids = set(state.get("sent_ids", []))
    
    print("æ­£åœ¨æŠ“å– AK æ¨èçš„åšå®¢...")
    articles = fetch_all_feeds()
    print(f"å…±æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    
    # è¿‡æ»¤å·²å‘é€çš„
    new_articles = [a for a in articles if a["id"] not in sent_ids]
    print(f"å…¶ä¸­æ–°æ–‡ç«  {len(new_articles)} ç¯‡")
    
    if not new_articles:
        print("æ²¡æœ‰æ–°æ–‡ç« ")
        return None
    
    # å–å‰Næ¡
    to_send = new_articles[:MAX_ITEMS]
    
    # æ ¼å¼åŒ–æ¶ˆæ¯
    message = format_feishu_message(to_send)
    
    # æ›´æ–°çŠ¶æ€
    for art in to_send:
        sent_ids.add(art["id"])
    
    state["sent_ids"] = list(sent_ids)[-200:]
    state["last_run"] = datetime.now().isoformat()
    save_state(state)
    
    return message

if __name__ == "__main__":
    msg = main()
    if msg:
        print(msg)
    else:
        print("No new articles to send")

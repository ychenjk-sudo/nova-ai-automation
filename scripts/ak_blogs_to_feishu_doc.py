#!/usr/bin/env python3
"""
AK åšå®¢ç²¾é€‰ â†’ é£ä¹¦æ–‡æ¡£
1. æŠ“å–æ–‡ç« 
2. è·å–å…¨æ–‡
3. AIæ‘˜è¦+ç¿»è¯‘
4. å†™å…¥é£ä¹¦æ–‡æ¡£
5. è¿”å›æ–‡æ¡£é“¾æ¥
"""

import feedparser
import json
import re
import os
import sys
from datetime import datetime, timedelta
from html import unescape
import concurrent.futures
import socket

# é…ç½®
STATE_FILE = "/workspace/scripts/.ak_blogs_doc_state.json"
MAX_ITEMS = 5
MAX_AGE_DAYS = 3

# RSS æºåˆ—è¡¨ï¼ˆç²¾é€‰ï¼‰
RSS_FEEDS = [
    ("Simon Willison", "https://simonwillison.net/atom/everything/"),
    ("Jeff Geerling", "https://www.jeffgeerling.com/blog.xml"),
    ("antirez", "http://antirez.com/rss"),
    ("Pluralistic", "https://pluralistic.net/feed/"),
    ("Mitchell Hashimoto", "https://mitchellh.com/feed.xml"),
    ("Xe Iaso", "https://xeiaso.net/blog.rss"),
    ("Gary Marcus", "https://garymarcus.substack.com/feed"),
    ("Dan Abramov", "https://overreacted.io/rss.xml"),
    ("matklad", "https://matklad.github.io/feed.xml"),
    ("Paul Graham", "http://www.aaronsw.com/2002/feeds/pgessays.rss"),
    ("Julia Evans", "https://jvns.ca/atom.xml"),
    ("Stratechery", "https://stratechery.com/feed/"),
    ("fasterthanli.me", "https://fasterthanli.me/index.xml"),
    ("Drew DeVault", "https://drewdevault.com/blog/index.xml"),
    ("Coding Horror", "https://blog.codinghorror.com/rss/"),
    ("Hacker News Top", "https://hnrss.org/frontpage"),
]

def load_state():
    try:
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    except:
        return {"sent_ids": [], "last_run": None}

def save_state(state):
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f, indent=2)

def clean_html(html_text):
    if not html_text:
        return ""
    text = re.sub(r'<[^>]+>', '', str(html_text))
    text = unescape(text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def fetch_single_feed(feed_info):
    """æŠ“å–å•ä¸ª RSS æº"""
    name, url = feed_info
    socket.setdefaulttimeout(8)
    try:
        feed = feedparser.parse(url)
        articles = []
        cutoff = datetime.now() - timedelta(days=MAX_AGE_DAYS)
        
        for entry in feed.entries[:3]:
            pub_date = None
            for date_field in ['published', 'updated', 'created']:
                if hasattr(entry, date_field + '_parsed') and getattr(entry, date_field + '_parsed'):
                    try:
                        pub_date = datetime(*getattr(entry, date_field + '_parsed')[:6])
                        break
                    except:
                        pass
            
            if pub_date and pub_date.replace(tzinfo=None) < cutoff:
                continue
            
            title = entry.get('title', '')
            link = entry.get('link', '')
            # è·å–å®Œæ•´å†…å®¹
            content = entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
            if not content:
                content = entry.get('summary', entry.get('description', ''))
            
            content_text = clean_html(content)
            
            if title and link:
                articles.append({
                    "id": link,
                    "title": title,
                    "link": link,
                    "source": name,
                    "content": content_text[:5000],  # é™åˆ¶é•¿åº¦
                    "pub_date": pub_date.isoformat() if pub_date else None
                })
        
        return articles
    except Exception as e:
        return []

def fetch_all_feeds():
    """å¹¶å‘æŠ“å–"""
    all_articles = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
        results = executor.map(fetch_single_feed, RSS_FEEDS)
        for articles in results:
            all_articles.extend(articles)
    
    all_articles.sort(key=lambda x: x.get('pub_date') or '', reverse=True)
    return all_articles

def format_for_doc(articles):
    """æ ¼å¼åŒ–ä¸ºé£ä¹¦æ–‡æ¡£ Markdown"""
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    lines = [
        f"# ğŸ“š AK åšå®¢ç²¾é€‰ ({today})",
        "",
        f"> æ¥æº: Karpathy æ¨èçš„ HN çƒ­é—¨åšå®¢ | å…± {len(articles)} ç¯‡",
        "",
        "---",
        "",
    ]
    
    for i, art in enumerate(articles, 1):
        lines.append(f"## {i}. {art['title']}")
        lines.append("")
        lines.append(f"**æ¥æº**: {art['source']} | **é“¾æ¥**: {art['link']}")
        lines.append("")
        
        if art.get('summary_zh'):
            lines.append("### ğŸ“Œ æ ¸å¿ƒè§‚ç‚¹")
            lines.append("")
            lines.append(art['summary_zh'])
            lines.append("")
        
        if art.get('translation'):
            lines.append("### ğŸ“– å†…å®¹ç¿»è¯‘")
            lines.append("")
            lines.append(art['translation'])
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    return "\n".join(lines)

def main():
    """ä¸»å‡½æ•° - è¿”å›æ–‡ç« æ•°æ®ä¾›å¤–éƒ¨å¤„ç†"""
    state = load_state()
    sent_ids = set(state.get("sent_ids", []))
    
    print("æ­£åœ¨æŠ“å– AK æ¨èåšå®¢...", file=sys.stderr)
    articles = fetch_all_feeds()
    print(f"å…±æŠ“å–åˆ° {len(articles)} ç¯‡", file=sys.stderr)
    
    new_articles = [a for a in articles if a["id"] not in sent_ids]
    print(f"å…¶ä¸­æ–°æ–‡ç«  {len(new_articles)} ç¯‡", file=sys.stderr)
    
    if not new_articles:
        print("æ²¡æœ‰æ–°æ–‡ç« ", file=sys.stderr)
        return None
    
    to_process = new_articles[:MAX_ITEMS]
    
    # æ›´æ–°çŠ¶æ€
    for art in to_process:
        sent_ids.add(art["id"])
    state["sent_ids"] = list(sent_ids)[-200:]
    state["last_run"] = datetime.now().isoformat()
    save_state(state)
    
    # è¾“å‡º JSON ä¾›å¤–éƒ¨å¤„ç†
    print(json.dumps(to_process, ensure_ascii=False, indent=2))
    return to_process

if __name__ == "__main__":
    main()
